docker run -i -t -p 8020:8020 -p 8888:8888 -p 11000:11000 -p 11443:11443 -p 9090:9090 -p 8088:8088 -p 19888:19888 -p 8983:8983 -p 16000:16000 -p 16001:16001 -p 42222:22 -p 8042:8042 -p 60010:60010 -p 50070:50070 -p 9000:9000 caioquirino/docker-cloudera-quickstart
beeline --incremental=true
!connect jdbc:hive2://localhost:10000/default

beeline --incremental=true -u jdbc:hive2://localhost:10000/default -n user -p pwd

docker run -i -t  caioquirino/docker-cloudera-quickstart
beeline --incremental=true
!connect jdbc:hive2://localhost:10000/default


/*set yarn.scheduler.maximum-allocation-mb=1024;
set yarn.app.mapreduce.am.resource.mb=512;
  
insert into table test values (1)*/

su - hdfs
hdfs dfs -chmod 777 /user/hive/warehouse/

set mapreduce.map.memory.mb=128;
set yarn.nodemanager.resource.memory-mb=256;
set yarn.scheduler.maximum-allocation-mb=512;
insert into table test values (1);

1)starting zookeeper
cd personal_git/docker/dockermfdknfdk/
docker-compose -f zk_single_single.yml up

kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic numtest --create --partitions 3 --replication-factor 1

mongo localhost/numtest

running mongo connect :
connect-standalone.sh connect-standalone.properties mongo.properties

connect-standalone.properties must contain the plugin.path which contains the files downloaded from confluent (connector)
mongo.properties must contain in the name of the collection the localhost:27017/database

mongo localhost/database checks the database
db.collection.find() search through records in collection

docker run -d -p 8080:8080 puckel/docker-airflow webserver